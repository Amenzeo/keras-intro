{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/title.png\" width=\"95%\"/></center>\n",
    "<center><a href=\"http://bit.ly/pybay-keras\">bit.ly/pybay-keras</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Who Am I?\n",
    "-----\n",
    "\n",
    "<center>Brian Spiering</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What do I do?\n",
    "------\n",
    "\n",
    "<center>Professor</center>\n",
    "<center><img src=\"images/msds_logo.png\" width=\"30%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/real_deep_learning.png\" width=\"80%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Keras - Neural Networks for humans\n",
    "------\n",
    "\n",
    "<center><img src=\"images/keras-logo-small.jpg\" width=\"25%\"/></center>\n",
    "\n",
    "A higher-level, more intuitive API for Deep Learning.\n",
    "\n",
    "Easy to define neural networks, then automatically handles execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A simple and highly modular interface which allows focus on learning and enabling fast experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Deep Learning 101\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/neural_nets.jpg\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Deep Learning (DL) are Neural networks (NN) with >1 hidden layer\n",
    "-------\n",
    "\n",
    "<center><img src=\"images/neural-networks-layers.jpg\" width=\"80%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Neural Networks are Nodes & Edges\n",
    "------\n",
    "<center><img src=\"images/sum.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Nonlinear function allows learning of nonlinear relationships\n",
    "------\n",
    "\n",
    "<center><img src=\"images/function_3.png\" width=\"80%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Groups of nodes all the way down\n",
    "------\n",
    "\n",
    "<center><img src=\"images/layers.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Deep Learning isn't magic, it is just very good at finding patterns.\n",
    "------\n",
    "\n",
    "<center><img src=\"images/features.png\" width=\"80%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Deep Learning has fewer steps than traditional Machine Learning\n",
    "------\n",
    "\n",
    "<center><img src=\"images/traditional-ml-deep-learning-2.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you want to follow along…\n",
    "-----\n",
    "\n",
    "GitHub repo: [bit.ly/pybay-keras](http://bit.ly/pybay-keras)\n",
    "\n",
    "If you want to type along…\n",
    "------\n",
    "\n",
    "1. Run a local Jupyter Notebook\n",
    "1. [Binder](https://mybinder.org/v2/gh/brianspiering/keras-intro/master): In-Browser Jupyter Notebook\n",
    "1. [Colaboratory](https://colab.research.google.com/): \"Google Docs for Jupyter Notebooks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# What is the backend / execution engine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tensorflow'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.backend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/tf_logo.jpg\" width=\"70%\"/></center>\n",
    "\n",
    "\"An open-source software library for Machine Intelligence\"\n",
    "\n",
    "Numerical computation using data flow graphs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "TensorFlow: A great backend\n",
    "------\n",
    "A __very__ flexible architecture that allows you to do almost any operation.\n",
    "\n",
    "Then deploy the computation to CPUs (one or more) or GPUs across desktop, cloud, or mobile device. \n",
    "<center><img src=\"images/tf_features.png\" width=\"45%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "MNIST handwritten digit database: <br> The “Hello World!” of Computer Vision\n",
    "------\n",
    "\n",
    "<center><img src=\"images/mnist-digits.png\" width=\"80%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/MNIST-Matrix.png\" width=\"80%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/MNIST_neuralnet_image.png\" width=\"80%\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Import data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Setup train and test splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from matplotlib import pyplot\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.imshow(x_train[randint(0, x_train.shape[0])], cmap='gray_r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Munge data\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/mnist_keras.png\" width=\"75%\"/></center>\n",
    "\n",
    "Convert image matrix into vector to feed into first layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Convert matrix to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "image_size = 784 # 28 x 28\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], image_size) # Transform from matrix to vector\n",
    "x_train = x_train.astype('float32') # Cast as 32 bit integers\n",
    "x_train /= 255 # Normalize inputs from 0-255 to 0.0-1.0\n",
    "\n",
    "x_test = x_test.reshape(x_test.shape[0], image_size) # Transform from matrix to vector\n",
    "x_test = x_test.astype('float32') # Cast as 32 bit integers\n",
    "x_test /= 255 # Normalize inputs from 0-255 to 0.0-1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Import the most common type of neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "RTFM - https://keras.io/layers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define model instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Import the most common type of network layer, fulling interconneced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"images/dense.png\" width=\"55%\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define input layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "layer_input = Dense(units=512, # Number of nodes\n",
    "                    activation='sigmoid', # The nonlinearity\n",
    "                    input_shape=(image_size,)) \n",
    "model.add(layer_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define another layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "model.add(Dense(units=512, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define output layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "layer_output = Dense(units=10, # Number of digits (0-9)\n",
    "                     activation='softmax') # Convert neural activation to probability of category\n",
    "\n",
    "model.add(layer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Yes - we compile the model to run it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/5\n",
      "54000/54000 [==============================] - 14s 255us/step - loss: 2.1430 - acc: 0.3381 - val_loss: 1.8788 - val_acc: 0.5865\n",
      "Epoch 2/5\n",
      "54000/54000 [==============================] - 13s 246us/step - loss: 1.4831 - acc: 0.6650 - val_loss: 1.0612 - val_acc: 0.7767\n",
      "Epoch 3/5\n",
      "54000/54000 [==============================] - 13s 245us/step - loss: 0.8919 - acc: 0.7898 - val_loss: 0.6704 - val_acc: 0.8498\n",
      "Epoch 4/5\n",
      "54000/54000 [==============================] - 13s 248us/step - loss: 0.6475 - acc: 0.8376 - val_loss: 0.5063 - val_acc: 0.8808\n",
      "Epoch 5/5\n",
      "54000/54000 [==============================] - 16s 301us/step - loss: 0.5343 - acc: 0.8579 - val_loss: 0.4244 - val_acc: 0.8933\n"
     ]
    }
   ],
   "source": [
    "# Define training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "training = model.fit(x_train, \n",
    "                     y_train,\n",
    "                     epochs=5, # Number of passes over complete dataset\n",
    "                     verbose=True, \n",
    "                     validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/waiting.jpg\" width=\"50%\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 94us/step\n",
      "Test loss: 0.475\n",
      "Test accuracy: 87.440%\n"
     ]
    }
   ],
   "source": [
    "# Let's see how good we did\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_test, \n",
    "                                y_test, \n",
    "                                verbose=True)\n",
    "print(f\"Test loss: {loss:.3}\")\n",
    "print(f\"Test accuracy: {accuracy:.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Keras' Other Features\n",
    "-----\n",
    "\n",
    "- Common built-in functions (e.g., activation functions and optimitizers)\n",
    "- Convolutional neural network (CNN or ConvNet)\n",
    "- Recurrent neural network (RNN) & Long-short term memory (LSTM)\n",
    "- Pre-trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "-----\n",
    "\n",
    "- Keras is designed for human beings, not computers.\n",
    "- Simple to define complex neural networks.\n",
    "- Easier to learn about Deep Learning (focus on the __what__, not the __how__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/twitter.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Futher Study - Keras\n",
    "--------\n",
    "\n",
    "- Keras docs\n",
    "    - https://keras.io/\n",
    "    - https://blog.keras.io/\n",
    "- Keras courses\n",
    "    - https://www.edx.org/course/deep-learning-fundamentals-with-keras\n",
    "    - https://www.coursera.org/lecture/ai/keras-overview-7GfN9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Futher Study - Deep Learning\n",
    "--------\n",
    "\n",
    "- Linear Algebra, Probability, Machine Learning\n",
    "- Study Deep Learning\n",
    "    - [fastai Course](http://www.fast.ai/)\n",
    "    - [Deep Learning Book](http://www.deeplearningbook.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bonus Material\n",
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from keras import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable          Type          Data/Info\n",
      "-----------------------------------------\n",
      "Dense             type          <class 'keras.layers.core.Dense'>\n",
      "Input             function      <function Input at 0x120fe18c8>\n",
      "Model             type          <class 'keras.engine.training.Model'>\n",
      "Sequential        type          <class 'keras.engine.sequential.Sequential'>\n",
      "absolute_import   _Feature      _Feature((2, 5, 0, 'alpha<...>0, 0, 'alpha', 0), 16384)\n",
      "accuracy          float64       0.8744\n",
      "activations       module        <module 'keras.activation<...>es/keras/activations.py'>\n",
      "applications      module        <module 'keras.applicatio<...>pplications/__init__.py'>\n",
      "backend           module        <module 'keras.backend' f<...>ras/backend/__init__.py'>\n",
      "callbacks         module        <module 'keras.callbacks'<...>ages/keras/callbacks.py'>\n",
      "constraints       module        <module 'keras.constraint<...>es/keras/constraints.py'>\n",
      "datasets          module        <module 'keras.datasets' <...>as/datasets/__init__.py'>\n",
      "engine            module        <module 'keras.engine' fr<...>eras/engine/__init__.py'>\n",
      "image_size        int           784\n",
      "initializers      module        <module 'keras.initialize<...>s/keras/initializers.py'>\n",
      "keras             module        <module 'keras' from '/Us<...>kages/keras/__init__.py'>\n",
      "layer_input       Dense         <keras.layers.core.Dense object at 0x132e8c6a0>\n",
      "layer_output      Dense         <keras.layers.core.Dense object at 0x13864d550>\n",
      "layers            module        <module 'keras.layers' fr<...>eras/layers/__init__.py'>\n",
      "legacy            module        <module 'keras.legacy' fr<...>eras/legacy/__init__.py'>\n",
      "loss              float64       0.4747597848176956\n",
      "losses            module        <module 'keras.losses' fr<...>ackages/keras/losses.py'>\n",
      "metrics           module        <module 'keras.metrics' f<...>ckages/keras/metrics.py'>\n",
      "mnist             module        <module 'keras.datasets.m<...>keras/datasets/mnist.py'>\n",
      "model             Sequential    <keras.engine.sequential.<...>al object at 0x133ab07f0>\n",
      "models            module        <module 'keras.models' fr<...>ackages/keras/models.py'>\n",
      "optimizers        module        <module 'keras.optimizers<...>ges/keras/optimizers.py'>\n",
      "preprocessing     module        <module 'keras.preprocess<...>eprocessing/__init__.py'>\n",
      "pyplot            module        <module 'matplotlib.pyplo<...>es/matplotlib/pyplot.py'>\n",
      "randint           method        <bound method Random.rand<...>bject at 0x7ff8e781e618>>\n",
      "regularizers      module        <module 'keras.regularize<...>s/keras/regularizers.py'>\n",
      "training          History       <keras.callbacks.History object at 0x1386fc588>\n",
      "utils             module        <module 'keras.utils' fro<...>keras/utils/__init__.py'>\n",
      "wrappers          module        <module 'keras.wrappers' <...>as/wrappers/__init__.py'>\n",
      "x_test            ndarray       10000x784: 7840000 elems, type `float32`, 31360000 bytes (29.9072265625 Mb)\n",
      "x_train           ndarray       60000x784: 47040000 elems, type `float32`, 188160000 bytes (179.443359375 Mb)\n",
      "y_test            ndarray       10000x10: 100000 elems, type `float32`, 400000 bytes (390.625 kb)\n",
      "y_train           ndarray       60000x10: 600000 elems, type `float32`, 2400000 bytes (2.288818359375 Mb)\n"
     ]
    }
   ],
   "source": [
    "whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Setup train and test splits\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.imshow(x_train[randint(0, x_train.shape[0])], cmap='gray_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define CNN model\n",
    "\n",
    "# Redefine input dimensions to make sure conv works\n",
    "img_rows, img_cols = 28, 28\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, \n",
    "             kernel_size=(3, 3),\n",
    "             activation='sigmoid',\n",
    "             input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='sigmoid'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/5\n",
      "54000/54000 [==============================] - 208s 4ms/step - loss: 1.0044 - acc: 0.7066 - val_loss: 0.3622 - val_acc: 0.8710\n",
      "Epoch 2/5\n",
      "54000/54000 [==============================] - 205s 4ms/step - loss: 0.2965 - acc: 0.8937 - val_loss: 0.2828 - val_acc: 0.8968\n",
      "Epoch 3/5\n",
      " 6080/54000 [==>...........................] - ETA: 2:44 - loss: 0.2287 - acc: 0.9164"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-d6720551ed22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                      \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                      \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                      validation_split=0.1)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define training\n",
    "training = model.fit(x_train, \n",
    "                     y_train,\n",
    "                     epochs=5,\n",
    "                     verbose=True, \n",
    "                     validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_test, \n",
    "                                y_test, \n",
    "                                verbose=True)\n",
    "print(f\"Test loss: {loss:.3}\")\n",
    "print(f\"Test accuracy: {accuracy:.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is `keras`?    \n",
    "-----\n",
    "\n",
    "<center><img src=\"https://www.thevintagenews.com/wp-content/uploads/2017/08/a-drinking-horn-from-the-16th-century-known-as-the-roordahuizum-drinking-horn-on-display-in-the-frisian-museum-at-leeuwarden-640x360.jpg\" width=\"75%\"/></center>\n",
    "\n",
    "Keras (κέρας) means horn in Greek. It is a reference to a literary image from ancient Greek and Latin literature, first found in the Odyssey, where dream spirits (Oneiroi, singular Oneiros) are divided between those who deceive men with false visions, who arrive to Earth through a gate of ivory, and those who announce a future that will come to pass, who arrive through a gate of horn. It's a play on the words κέρας (horn) / κραίνω (fulfill), and ἐλέφας (ivory) / ἐλεφαίρομαι (deceive).\n",
    "\n",
    "[Source](https://keras.io/#why-this-name-keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
